{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Disable the SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def participant_fixations_to_df(path_to_txt_file: str) -> pd.DataFrame:\n",
    "    df_participant_fixations = pd.read_csv(path_to_txt_file, sep='\\t')\n",
    "\n",
    "    # convert str to list\n",
    "    df_participant_fixations['CURRENT_FIX_INTEREST_AREAS'] = df_participant_fixations[\n",
    "        'CURRENT_FIX_INTEREST_AREAS'\n",
    "    ].apply(json.loads)\n",
    "\n",
    "    # add accuracy column, accuracy defines whether the comprehension task was answered correctly\n",
    "    # 1 == correct, 0 == wrong\n",
    "    df_participant_fixations['accuracy'] = df_participant_fixations.apply(\n",
    "        lambda row: 1 if (row.correct_option == row.KEY_STROKE) else 0, axis=1,\n",
    "    )\n",
    "\n",
    "    # keep only fixations on code snippet (remove those on task / answer options)\n",
    "    df_participant_fixations = df_participant_fixations[df_participant_fixations['CURRENT_FIX_X'].between(430, 1400)]  # noqa: E501\n",
    "\n",
    "    df_participant_fixations = df_participant_fixations[[\n",
    "        'RECORDING_SESSION_LABEL', 'code_snippet_id', 'CURRENT_FIX_INDEX', 'CURRENT_FIX_DURATION',\n",
    "        'CURRENT_FIX_INTEREST_AREAS', 'CURRENT_FIX_NEAREST_INTEREST_AREA', 'accuracy',\n",
    "        'CS_SUBJ_DIFFICULTY',\n",
    "    ]]\n",
    "\n",
    "    return df_participant_fixations\n",
    "\n",
    "\n",
    "def get_code_snippet_df(\n",
    "    df_participant_fixations: pd.DataFrame,\n",
    "    code_snippet_id: str,\n",
    ") -> pd.DataFrame:\n",
    "    # select code snippet\n",
    "    participant_snippet_fixations = df_participant_fixations[\n",
    "        df_participant_fixations['code_snippet_id'] == code_snippet_id\n",
    "    ].copy()\n",
    "\n",
    "    # fix spelling error in data for 10-177-V1/V2\n",
    "    if len(participant_snippet_fixations) == 0 and code_snippet_id[:6] == '10-177':\n",
    "        code_snippet_id_fixed = code_snippet_id[:4] + '17-V' + code_snippet_id[-1:]  # 10-117-V\n",
    "\n",
    "        participant_snippet_fixations = df_participant_fixations[\n",
    "            df_participant_fixations['code_snippet_id'] == code_snippet_id_fixed\n",
    "        ].copy()\n",
    "\n",
    "    return participant_snippet_fixations\n",
    "\n",
    "\n",
    "def get_IA_df(code_snippet_id: str) -> pd.DataFrame:\n",
    "    df_IA = pd.read_csv(\n",
    "        filepath_or_buffer=f'code_snippets/IAs/IA-{code_snippet_id}.ias', skiprows=2, sep='\\t',\n",
    "        quoting=csv.QUOTE_NONE, names=['FORM', 'IA_ID', 'C1', 'C2', 'C3', 'C4', 'LABEL'],\n",
    "    )\n",
    "    return df_IA\n",
    "\n",
    "\n",
    "def code_tokenizer(\n",
    "    code_snippet_id: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> list:\n",
    "    # tokenize code\n",
    "    f = open(f'code_snippets/original_naming/{code_snippet_id}.py')\n",
    "    code = f.read()\n",
    "\n",
    "    # cut off 366-143-V1 because it is too long for CodeBERT Input\n",
    "    if code_snippet_id == '366-143-V1':\n",
    "        code_tokens = tokenizer.tokenize(code[:-155])\n",
    "    else:\n",
    "        code_tokens = tokenizer.tokenize(code)\n",
    "\n",
    "    return code_tokens\n",
    "\n",
    "\n",
    "def get_code_token_embeddings(\n",
    "    code_tokens: np.array,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModel,\n",
    ") -> np.array:\n",
    "    # covert tokens to ids\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(code_tokens)\n",
    "\n",
    "    # create code token embeddings\n",
    "    context_embeddings = model(torch.tensor(tokens_ids)[None, :])[0]\n",
    "\n",
    "    return context_embeddings\n",
    "\n",
    "\n",
    "def map_embeddings_to_IAs(code_tokens: list, df_IA: pd.DataFrame) -> pd.DataFrame:\n",
    "    # map token embedding indices to Interest Areas\n",
    "    strip_chars = 'ĊĠ '\n",
    "\n",
    "    l_i = 0\n",
    "    t_i = 0\n",
    "\n",
    "    df_tmp = df_IA.copy()\n",
    "    df_IA['embedding_idxs'] = None\n",
    "\n",
    "    for token in code_tokens:\n",
    "        token = str(token).strip(strip_chars)\n",
    "        df_tmp['LABEL'].iloc[l_i] = str(df_tmp['LABEL'].iloc[l_i]).strip(strip_chars)\n",
    "\n",
    "        if len(token) == 0:\n",
    "            # skip whitespace tokens\n",
    "            t_i += 1\n",
    "            continue\n",
    "\n",
    "        if token == df_tmp['LABEL'].iloc[l_i]:\n",
    "            if df_IA['embedding_idxs'].iloc[l_i] is None:\n",
    "                df_IA['embedding_idxs'].iloc[l_i] = [t_i]\n",
    "            else:\n",
    "                df_IA['embedding_idxs'].iloc[l_i].append(t_i)\n",
    "\n",
    "            l_i += 1\n",
    "            t_i += 1\n",
    "\n",
    "        elif token in df_tmp['LABEL'].iloc[l_i]:\n",
    "            if df_IA['embedding_idxs'].iloc[l_i] is None:\n",
    "                df_IA['embedding_idxs'].iloc[l_i] = [t_i]\n",
    "            else:\n",
    "                df_IA['embedding_idxs'].iloc[l_i].append(t_i)\n",
    "\n",
    "            # removed processed token from IA label\n",
    "            df_tmp['LABEL'].iloc[l_i] = df_tmp['LABEL'].iloc[l_i].replace(token, '', 1)\n",
    "\n",
    "            if len(df_tmp['LABEL'].iloc[l_i]) == 0:\n",
    "                # no more tokens left in IA label\n",
    "                l_i += 1\n",
    "                t_i += 1\n",
    "            else:\n",
    "                # IA label still contains tokens\n",
    "                t_i += 1\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Error at t_i = {t_i}; l_i = {l_i}; for token {token} and label \\n {df_tmp['LABEL'].iloc[l_i - 1]} \\n >>> {df_tmp['LABEL'].iloc[l_i]} \\n {df_tmp['LABEL'].iloc[l_i + 1]}\")  # noqa: E501\n",
    "\n",
    "    return df_IA.copy()\n",
    "\n",
    "\n",
    "def map_embeddings_to_fixations(\n",
    "    participant_snippet_fixations: pd.DataFrame,\n",
    "    df_mapped_IA: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # map embedding indicies to fixations\n",
    "\n",
    "    participant_snippet_fixations['embedding_idxs'] = None\n",
    "    for i, (j, row) in enumerate(participant_snippet_fixations.iterrows()):\n",
    "        # get Interest Area ID or nearest Interest Area ID\n",
    "        IA_ID = row['CURRENT_FIX_INTEREST_AREAS'][0] if len(row['CURRENT_FIX_INTEREST_AREAS']) > 0 else row['CURRENT_FIX_NEAREST_INTEREST_AREA']  # noqa: E501\n",
    "\n",
    "        # get embedding_idxs for fixation's Interest Area\n",
    "        participant_snippet_fixations['embedding_idxs'].iloc[i] = df_mapped_IA[df_mapped_IA['IA_ID'] == int(IA_ID)][  # noqa: E501\n",
    "            'embedding_idxs'\n",
    "        ].values.tolist()\n",
    "\n",
    "        # flatten embedding_idxs Array to 1D\n",
    "        participant_snippet_fixations['embedding_idxs'] = participant_snippet_fixations['embedding_idxs'].apply(  # noqa: E501\n",
    "            np.ravel,\n",
    "        )\n",
    "\n",
    "    return participant_snippet_fixations.copy()\n",
    "\n",
    "\n",
    "def compute_input_matrix_row(\n",
    "    participant_snippet_mapped_fixations: pd.DataFrame, context_embeddings: torch.Tensor,\n",
    "    add_labels_fix_feature: bool = False,\n",
    ") -> list[str | int | float]:\n",
    "    # compute input matrix\n",
    "    EMBED_VECTOR_SIZE = 768\n",
    "    input_matrix_row = []\n",
    "    # input_matrix_row_meta = []\n",
    "\n",
    "    for i, row in participant_snippet_mapped_fixations.iterrows():\n",
    "        # init Interest Area embedding\n",
    "        IA_embedding = np.zeros(EMBED_VECTOR_SIZE)\n",
    "\n",
    "        # Sum up all token embeddings of respective Interest Area\n",
    "        for embedding_idx in row['embedding_idxs']:\n",
    "            # skip Interest Area if its tokens are beyond CodeBert's max length input\n",
    "            if embedding_idx is None:\n",
    "                continue\n",
    "            IA_tk_embedding = context_embeddings[0][embedding_idx].detach().numpy()\n",
    "            # print(IA_tk_embedding.shape)\n",
    "            IA_embedding += IA_tk_embedding\n",
    "\n",
    "        # skip Interest Area if its tokens are beyond CodeBert's max length input\n",
    "        if np.array_equal(IA_embedding, np.zeros(EMBED_VECTOR_SIZE)):\n",
    "            continue\n",
    "\n",
    "        # divide by token amount of Interest area to get an Interest Area embedding\n",
    "        # that is the average of all it's tokens\n",
    "        IA_embedding /= len(row['embedding_idxs'])\n",
    "\n",
    "        IA_ID = int(row['CURRENT_FIX_NEAREST_INTEREST_AREA'])\n",
    "        FIX_DUR = row['CURRENT_FIX_DURATION']\n",
    "\n",
    "        input_matrix_row += [IA_ID, FIX_DUR] + IA_embedding.tolist()\n",
    "\n",
    "    accuracy = participant_snippet_mapped_fixations['accuracy'].iloc[0]\n",
    "    code_snippet_id = participant_snippet_mapped_fixations['code_snippet_id'].iloc[0]\n",
    "    participant_id = participant_snippet_mapped_fixations['RECORDING_SESSION_LABEL'].iloc[0]\n",
    "    subjective_difficulty = participant_snippet_mapped_fixations['CS_SUBJ_DIFFICULTY'].iloc[0]\n",
    "\n",
    "    max_sequence_length = 1126\n",
    "    max_sequence_with_embed_len = (EMBED_VECTOR_SIZE + 2) * max_sequence_length\n",
    "    padded_input_matrix_row = np.pad(input_matrix_row, (0, max_sequence_with_embed_len - len(input_matrix_row)), mode='constant').tolist()\n",
    "    if add_labels_fix_feature:\n",
    "        return [code_snippet_id, participant_id, accuracy, subjective_difficulty] + padded_input_matrix_row\n",
    "    else:\n",
    "        return padded_input_matrix_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_embedding_lookup_table(\n",
    "    IA_table: pd.DataFrame, codebert_embedding: torch.Tensor, graphcodebert_embedding: torch.Tensor,\n",
    "):\n",
    "    EMBED_VECTOR_SIZE = 768\n",
    "    output_df = IA_table.copy()\n",
    "    output_df[\"codebert_code_embedding\"] = [[] for _ in range(len(output_df))]\n",
    "    output_df[\"graphcodebert_code_embedding\"] = [[] for _ in range(len(output_df))]\n",
    "\n",
    "    for i, row in output_df.iterrows():\n",
    "        # init Interest Area embedding\n",
    "        IA_embedding_codebert = np.zeros(EMBED_VECTOR_SIZE)\n",
    "        IA_embedding_graphcodebert = np.zeros(EMBED_VECTOR_SIZE)\n",
    "\n",
    "        # Sum up all token embeddings of respective Interest Area\n",
    "        #print(row)\n",
    "\n",
    "        if row[\"embedding_idxs\"] == None:\n",
    "            print(f\"embedding_idxs was None for {row['LABEL']}\")\n",
    "            IA_embedding_codebert = IA_embedding_graphcodebert = np.zeros(EMBED_VECTOR_SIZE)\n",
    "            output_df.at[i,\"codebert_code_embedding\"] = IA_embedding_codebert.tolist()\n",
    "            output_df.at[i,\"graphcodebert_code_embedding\"] = IA_embedding_graphcodebert.tolist()\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            for embedding_idx in row['embedding_idxs']:\n",
    "                # skip Interest Area if its tokens are beyond CodeBert's max length input\n",
    "                if embedding_idx is None:\n",
    "                    continue\n",
    "                IA_tk_embedding_codebert = codebert_embedding[0][embedding_idx].detach().numpy()\n",
    "                IA_embedding_graphcodebert = graphcodebert_embedding[0][embedding_idx].detach().numpy()\n",
    "\n",
    "                IA_embedding_codebert += IA_tk_embedding_codebert\n",
    "                IA_embedding_graphcodebert += IA_embedding_graphcodebert\n",
    "\n",
    "        # divide by token amount of Interest area to get an Interest Area embedding\n",
    "        # that is the average of all it's tokens\n",
    "        IA_embedding_codebert /= len(row['embedding_idxs'])\n",
    "        IA_embedding_graphcodebert /= len(row['embedding_idxs'])\n",
    "\n",
    "        output_df.at[i,\"codebert_code_embedding\"] = IA_embedding_codebert.tolist()\n",
    "        output_df.at[i,\"graphcodebert_code_embedding\"] = IA_embedding_graphcodebert.tolist()\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw 10-258-V2 from P202\n",
      "Saw 49-510-V1 from P202\n",
      "Saw 142-929-V2 from P202\n",
      "Saw 369-1404-V1 from P202\n",
      "Saw 10-928-V1 from P202\n",
      "Saw 366-143-V1 from P202\n",
      "Saw 10-117-V1 from P202\n",
      "Saw A34-6 from P001\n",
      "Saw A49-7086 from P001\n",
      "Saw A189-895 from P001\n",
      "Saw A1117-384 from P001\n",
      "Saw A1117-2696 from P001\n",
      "Saw A84-600 from P001\n",
      "Saw 369-1404-V1 from P001\n",
      "Saw 10-258-V2 from P203\n",
      "Saw 49-510-V1 from P203\n",
      "Saw 142-929-V2 from P203\n",
      "Saw 369-1404-V1 from P203\n",
      "Saw 10-928-V1 from P203\n",
      "Saw 366-143-V1 from P203\n",
      "Saw 10-117-V1 from P203\n",
      "Saw 10-258-V2 from P201\n",
      "Saw 49-510-V1 from P201\n",
      "Saw 142-929-V2 from P201\n",
      "Saw 369-1404-V1 from P201\n",
      "Saw 10-928-V1 from P201\n",
      "Saw 366-143-V1 from P201\n",
      "Saw 10-117-V1 from P201\n",
      "Saw 10-258-V2 from P204\n",
      "Saw 49-510-V1 from P204\n",
      "Saw 142-929-V2 from P204\n",
      "Saw 369-1404-V1 from P204\n",
      "Saw 10-928-V1 from P204\n",
      "Saw 366-143-V1 from P204\n",
      "Saw 10-117-V1 from P204\n",
      "Saw 10-258-V2 from P205\n",
      "Saw 49-510-V1 from P205\n",
      "Saw 142-929-V2 from P205\n",
      "Saw 369-1404-V1 from P205\n",
      "Saw 10-928-V1 from P205\n",
      "Saw 366-143-V1 from P205\n",
      "Saw 10-117-V1 from P205\n",
      "Saw A34-6 from P207\n",
      "Saw A49-7086 from P207\n",
      "Saw A189-895 from P207\n",
      "Saw A1117-384 from P207\n",
      "Saw A1117-2696 from P207\n",
      "Saw A84-600 from P207\n",
      "Saw 369-1404-V1 from P207\n",
      "Saw A34-6 from P206\n",
      "Saw A49-7086 from P206\n",
      "Saw A189-895 from P206\n",
      "Saw A1117-384 from P206\n",
      "Saw A1117-2696 from P206\n",
      "Saw A84-600 from P206\n",
      "Saw 369-1404-V1 from P206\n",
      "Saw 10-117-V2 from P103\n",
      "Saw 189-1871-V2 from P103\n",
      "Saw 10-181-V2 from P103\n",
      "Saw 49-510-V1 from P103\n",
      "Saw 10-258-V1 from P103\n",
      "Saw 366-143-V1 from P103\n",
      "Saw 10-258-V3 from P301\n",
      "Saw 10-117-V2 from P301\n",
      "Saw 142-929-V1 from P301\n",
      "Saw 49-76-V3 from P301\n",
      "Saw 10-181-V2 from P301\n",
      "Saw 49-510-V2 from P301\n",
      "Saw 10-928-V2 from P301\n",
      "Saw 366-143-V1 from P301\n",
      "Saw 189-1871-V1 from P301\n",
      "Saw A34-6 from P507\n",
      "Saw 10-258-V3 from P507\n",
      "Saw 49-76-V3 from P507\n",
      "Saw 10-181-V2 from P507\n",
      "Saw 49-510-V2 from P507\n",
      "Saw A1117-2696 from P507\n",
      "Saw 10-928-V2 from P507\n",
      "Saw 369-1404-V1 from P507\n",
      "Saw 366-143-V1 from P507\n",
      "Saw 189-1871-V1 from P507\n",
      "Saw 10-117-V2 from P102\n",
      "Saw 189-1871-V2 from P102\n",
      "Saw 10-181-V2 from P102\n",
      "Saw 49-510-V1 from P102\n",
      "Saw 10-258-V1 from P102\n",
      "Saw 366-143-V1 from P102\n",
      "Saw 10-258-V3 from P302\n",
      "Saw 10-117-V2 from P302\n",
      "Saw 142-929-V1 from P302\n",
      "Saw 49-76-V3 from P302\n",
      "Saw 10-181-V2 from P302\n",
      "Saw 49-510-V2 from P302\n",
      "Saw 10-928-V2 from P302\n",
      "Saw 366-143-V1 from P302\n",
      "Saw 189-1871-V1 from P302\n",
      "Saw A34-6 from P505\n",
      "Saw 10-258-V3 from P505\n",
      "Saw 49-76-V3 from P505\n",
      "Saw 10-181-V2 from P505\n",
      "Saw 49-510-V2 from P505\n",
      "Saw A1117-2696 from P505\n",
      "Saw 10-928-V2 from P505\n",
      "Saw 369-1404-V1 from P505\n",
      "Saw 366-143-V1 from P505\n",
      "Saw 189-1871-V1 from P505\n",
      "Saw 10-258-V3 from P303\n",
      "Saw 10-117-V2 from P303\n",
      "Saw 142-929-V1 from P303\n",
      "Saw 49-76-V3 from P303\n",
      "Saw 10-181-V2 from P303\n",
      "Saw 49-510-V2 from P303\n",
      "Saw 10-928-V2 from P303\n",
      "Saw 366-143-V1 from P303\n",
      "Saw 189-1871-V1 from P303\n",
      "Saw A34-6 from P101\n",
      "Saw A49-7086 from P101\n",
      "Saw A189-895 from P101\n",
      "Saw A1117-384 from P101\n",
      "Saw A1117-2696 from P101\n",
      "Saw A84-600 from P101\n",
      "Saw 369-1404-V1 from P101\n",
      "Saw A34-6 from P501\n",
      "Saw 10-258-V3 from P501\n",
      "Saw 49-76-V3 from P501\n",
      "Saw 10-181-V2 from P501\n",
      "Saw 49-510-V2 from P501\n",
      "Saw A1117-2696 from P501\n",
      "Saw 10-928-V2 from P501\n",
      "Saw 369-1404-V1 from P501\n",
      "Saw 366-143-V1 from P501\n",
      "Saw 189-1871-V1 from P501\n",
      "Saw 10-258-V3 from P306\n",
      "Saw 10-117-V2 from P306\n",
      "Saw 142-929-V1 from P306\n",
      "Saw 49-76-V3 from P306\n",
      "Saw 10-181-V2 from P306\n",
      "Saw 49-510-V2 from P306\n",
      "Saw 10-928-V2 from P306\n",
      "Saw 366-143-V1 from P306\n",
      "Saw 189-1871-V1 from P306\n",
      "Saw A34-6 from P104\n",
      "Saw 10-117-V2 from P104\n",
      "Saw 189-1871-V2 from P104\n",
      "Saw A49-7086 from P104\n",
      "Saw 10-181-V2 from P104\n",
      "Saw 49-510-V1 from P104\n",
      "Saw 10-258-V1 from P104\n",
      "Saw A1117-384 from P104\n",
      "Saw A1117-2696 from P104\n",
      "Saw A84-600 from P104\n",
      "Saw 369-1404-V1 from P104\n",
      "Saw 366-143-V1 from P104\n",
      "Saw 10-258-V3 from P304\n",
      "Saw 10-117-V2 from P304\n",
      "Saw 142-929-V1 from P304\n",
      "Saw 49-76-V3 from P304\n",
      "Saw 10-181-V2 from P304\n",
      "Saw 49-510-V2 from P304\n",
      "Saw 10-928-V2 from P304\n",
      "Saw 366-143-V1 from P304\n",
      "Saw 189-1871-V1 from P304\n",
      "Saw A34-6 from P502\n",
      "Saw 10-258-V3 from P502\n",
      "Saw 49-76-V3 from P502\n",
      "Saw 10-181-V2 from P502\n",
      "Saw 49-510-V2 from P502\n",
      "Saw A1117-2696 from P502\n",
      "Saw 10-928-V2 from P502\n",
      "Saw 369-1404-V1 from P502\n",
      "Saw 366-143-V1 from P502\n",
      "Saw 189-1871-V1 from P502\n",
      "Saw A34-6 from P503\n",
      "Saw 10-258-V3 from P503\n",
      "Saw 49-76-V3 from P503\n",
      "Saw 10-181-V2 from P503\n",
      "Saw 49-510-V2 from P503\n",
      "Saw A1117-2696 from P503\n",
      "Saw 10-928-V2 from P503\n",
      "Saw 369-1404-V1 from P503\n",
      "Saw 366-143-V1 from P503\n",
      "Saw 189-1871-V1 from P503\n",
      "Saw 10-258-V3 from P305\n",
      "Saw 10-117-V2 from P305\n",
      "Saw 142-929-V1 from P305\n",
      "Saw 49-76-V3 from P305\n",
      "Saw 10-181-V2 from P305\n",
      "Saw 49-510-V2 from P305\n",
      "Saw 10-928-V2 from P305\n",
      "Saw 366-143-V1 from P305\n",
      "Saw 189-1871-V1 from P305\n",
      "Saw A34-6 from P107\n",
      "Saw 10-117-V2 from P107\n",
      "Saw 189-1871-V2 from P107\n",
      "Saw A189-895 from P107\n",
      "Saw A49-7086 from P107\n",
      "Saw 10-181-V2 from P107\n",
      "Saw 49-510-V1 from P107\n",
      "Saw 10-258-V1 from P107\n",
      "Saw A1117-384 from P107\n",
      "Saw A1117-2696 from P107\n",
      "Saw A84-600 from P107\n",
      "Saw 369-1404-V1 from P107\n",
      "Saw 366-143-V1 from P107\n",
      "Saw A34-6 from P108\n",
      "Saw 10-117-V2 from P108\n",
      "Saw 189-1871-V2 from P108\n",
      "Saw A189-895 from P108\n",
      "Saw A49-7086 from P108\n",
      "Saw 10-181-V2 from P108\n",
      "Saw 49-510-V1 from P108\n",
      "Saw 10-258-V1 from P108\n",
      "Saw A1117-384 from P108\n",
      "Saw A1117-2696 from P108\n",
      "Saw A84-600 from P108\n",
      "Saw 369-1404-V1 from P108\n",
      "Saw 366-143-V1 from P108\n",
      "Saw A34-6 from P508\n",
      "Saw 10-258-V3 from P508\n",
      "Saw 49-76-V3 from P508\n",
      "Saw 10-181-V2 from P508\n",
      "Saw 49-510-V2 from P508\n",
      "Saw A1117-2696 from P508\n",
      "Saw 10-928-V2 from P508\n",
      "Saw 369-1404-V1 from P508\n",
      "Saw 366-143-V1 from P508\n",
      "Saw 189-1871-V1 from P508\n",
      "Saw 10-258-V2 from P208\n",
      "Saw 49-510-V1 from P208\n",
      "Saw 142-929-V2 from P208\n",
      "Saw 369-1404-V1 from P208\n",
      "Saw 10-928-V1 from P208\n",
      "Saw 366-143-V1 from P208\n",
      "Saw 10-117-V1 from P208\n",
      "Looking at 10-258-V3 from P208, idx: 0\n",
      "Looking at A1117-384 from P208, idx: 1\n",
      "Looking at 49-510-V1 from P208, idx: 2\n",
      "Looking at 142-929-V2 from P208, idx: 3\n",
      "Looking at 10-928-V1 from P208, idx: 4\n",
      "Looking at 366-143-V1 from P208, idx: 5\n",
      "embedding_idxs was None for elif\n",
      "embedding_idxs was None for isinstance\n",
      "embedding_idxs was None for (value,\n",
      "embedding_idxs was None for dict)\n",
      "embedding_idxs was None for or\n",
      "embedding_idxs was None for isinstance\n",
      "embedding_idxs was None for (value,\n",
      "embedding_idxs was None for list):\n",
      "embedding_idxs was None for out_str\n",
      "embedding_idxs was None for +=\n",
      "embedding_idxs was None for \"%s\n",
      "embedding_idxs was None for =>\n",
      "embedding_idxs was None for %s<br>\\n\"\n",
      "embedding_idxs was None for %\n",
      "embedding_idxs was None for (key,\n",
      "embedding_idxs was None for json.\n",
      "embedding_idxs was None for dumps\n",
      "embedding_idxs was None for (value))\n",
      "embedding_idxs was None for return\n",
      "embedding_idxs was None for out_str\n",
      "Looking at A49-7086 from P208, idx: 6\n",
      "Looking at A1117-2696 from P208, idx: 7\n",
      "Looking at 49-510-V2 from P208, idx: 8\n",
      "Looking at 369-1404-V1 from P208, idx: 9\n",
      "Looking at A34-6 from P208, idx: 10\n",
      "Looking at 189-1871-V2 from P208, idx: 11\n",
      "Looking at 10-181-V2 from P208, idx: 12\n",
      "Looking at 142-929-V1 from P208, idx: 13\n",
      "Looking at 49-76-V3 from P208, idx: 14\n",
      "Looking at 10-117-V1 from P208, idx: 15\n",
      "Looking at corrected ID 10-177-V1\n",
      "Looking at 189-1871-V1 from P208, idx: 16\n",
      "Looking at 10-258-V2 from P208, idx: 17\n",
      "Looking at 10-117-V2 from P208, idx: 18\n",
      "Looking at corrected ID 10-177-V2\n",
      "Looking at A189-895 from P208, idx: 19\n",
      "Looking at 10-258-V1 from P208, idx: 20\n",
      "Looking at 10-928-V2 from P208, idx: 21\n",
      "Looking at A84-600 from P208, idx: 22\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "model = AutoModel.from_pretrained('microsoft/codebert-base')\n",
    "graph_tokenizer = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')\n",
    "graph_model = AutoModel.from_pretrained('microsoft/graphcodebert-base')\n",
    "\n",
    "\n",
    "fix_report_parent_dir = './raw-fixation-reports'\n",
    "fix_report_parent_dir_additional = './raw-fixation-reports-add' # additional reports from participants present in fix_report_parent_dir\n",
    "\n",
    "fix_report_files = [\n",
    "    f for f in listdir(fix_report_parent_dir) if isfile(join(fix_report_parent_dir, f))\n",
    "]\n",
    "\n",
    "code_snippet_ids = set()\n",
    "participant_ids = set()\n",
    "\n",
    "embeddings: dict[str, dict[str, list[Any]]] = {}\n",
    "\n",
    "skip_snippets = [\n",
    "    '10-181-V3',\n",
    "    '10-928-V3',\n",
    "    '142-929-V3',\n",
    "    '189-1871-V3',\n",
    "    # '369-1404-V1', # regenerate reports using IAs for first round participants\n",
    "    '369-1404-V2',\n",
    "    '49-76-V1',  # missing IA file\n",
    "    '49-76-V2',  # missing IA file\n",
    "]\n",
    "\n",
    "for fix_report_file in fix_report_files:\n",
    "    participant_id = fix_report_file[-8:-4]\n",
    "\n",
    "    # build embeddings dict\n",
    "    participant_ids.add(participant_id)\n",
    "    if participant_id not in embeddings:\n",
    "        embeddings[participant_id] = {}\n",
    "\n",
    "    # get processed Participant Fixation DataFrame\n",
    "    df_participant_fixations = participant_fixations_to_df(\n",
    "        f'{fix_report_parent_dir}/{fix_report_file}',\n",
    "    )\n",
    "\n",
    "    # check if data from 2nd experiment round exists for participant and add it\n",
    "    if Path(f'./{fix_report_parent_dir_additional}/fix_report_{participant_id}.txt').exists():\n",
    "        df_participant_fixations_2 = participant_fixations_to_df(\n",
    "            f'./{fix_report_parent_dir_additional}/fix_report_{participant_id}.txt',\n",
    "        )\n",
    "        df_participant_fixations = pd.concat(\n",
    "            [df_participant_fixations, df_participant_fixations_2], join='inner',\n",
    "        )\n",
    "\n",
    "    # get Participant Code Snippet IDs and remove snippets to be skipped\n",
    "    participant_code_snippet_ids = set(df_participant_fixations['code_snippet_id'])\n",
    "    participant_code_snippet_ids = participant_code_snippet_ids - set(skip_snippets)\n",
    "\n",
    "    for code_snippet_id in participant_code_snippet_ids:\n",
    "        print(f'Saw {code_snippet_id} from {participant_id}')\n",
    "        code_snippet_ids.add(code_snippet_id)\n",
    "\n",
    "IA_LOOKUP_TABLE = {}\n",
    "\n",
    "# [[codebert_embedding_for_1st_cs, graphcodebert_embedding_for_1st_cs], [codebert_embedding_for_2nd_cs, graphcodebert_embedding_for_2nd_cs], ...]\n",
    "PRE_COMPUTED_EMBEDDINGS = []\n",
    "CS_ID_to_IDX_MAP = {}\n",
    "\n",
    "\n",
    "for i,code_snippet_id in enumerate(code_snippet_ids):\n",
    "    print(f'Looking at {code_snippet_id} from {participant_id}, idx: {i}')\n",
    "    # fix spelling error in data for 10-177-V1/V2\n",
    "    if code_snippet_id[:6] == '10-117':\n",
    "        code_snippet_id = code_snippet_id[:4] + '77-V' + code_snippet_id[-1:]  # 10-177-V\n",
    "        print(f'Looking at corrected ID {code_snippet_id}')\n",
    "\n",
    "    # generate mapping between CS_ID and CS_IDX\n",
    "    CS_ID_to_IDX_MAP[code_snippet_id] = i\n",
    "\n",
    "    # get processed participant_snippet_fixations DataFrame\n",
    "    participant_snippet_fixations = get_code_snippet_df(\n",
    "        df_participant_fixations, code_snippet_id,\n",
    "    )\n",
    "\n",
    "    # get Interest Areas as DataFrame\n",
    "    df_IA = get_IA_df(code_snippet_id)\n",
    "\n",
    "    # get tokenized code\n",
    "    bert_code_tokens = code_tokenizer(\n",
    "        code_snippet_id, tokenizer,\n",
    "    )\n",
    "    graph_bert_code_tokens = code_tokenizer(\n",
    "        code_snippet_id, graph_tokenizer,\n",
    "    )\n",
    "\n",
    "    # get code token embeddings\n",
    "    bert_context_embeddings = get_code_token_embeddings(\n",
    "        bert_code_tokens, tokenizer, model,\n",
    "    )\n",
    "\n",
    "    graph_bert_context_embeddings = get_code_token_embeddings(\n",
    "        graph_bert_code_tokens, graph_tokenizer, graph_model,\n",
    "    )\n",
    "\n",
    "    # map code token embeddings to Interest Area DataFrame\n",
    "    bert_df_mapped_IA = map_embeddings_to_IAs(bert_code_tokens, df_IA)\n",
    "    graph_bert_df_mapped_IA = map_embeddings_to_IAs(graph_bert_code_tokens, df_IA)\n",
    "\n",
    "    embedding_lookup_table = compute_embedding_lookup_table(bert_df_mapped_IA, bert_context_embeddings, graph_bert_context_embeddings)\n",
    "\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    tmp_embed_single_snippet = []\n",
    "    for _, row in embedding_lookup_table.iterrows():\n",
    "\n",
    "        # Extract the values from the specified columns\n",
    "        code_embedding = row[\"codebert_code_embedding\"]\n",
    "        graphcode_embedding = row[\"graphcodebert_code_embedding\"]\n",
    "\n",
    "        # Append the arrays as a tuple to the result list\n",
    "        tmp_embed_single_snippet.append((code_embedding, graphcode_embedding))\n",
    "\n",
    "    PRE_COMPUTED_EMBEDDINGS.append(tmp_embed_single_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PAD PRE_COMPUTED_EMBEDDINGS, s.t. all dimensions are of equal length\n",
    "\n",
    "# Determine the maximum length of the sublists\n",
    "max_length = max(len(sublist) for sublist in PRE_COMPUTED_EMBEDDINGS)\n",
    "\n",
    "# Pad each sublist with zeros to make them of consistent length\n",
    "for sublist in PRE_COMPUTED_EMBEDDINGS:\n",
    "    zero_embed = [0] * 768\n",
    "    sublist.extend([[zero_embed, zero_embed]] * (max_length - len(sublist)))\n",
    "\n",
    "np.array(PRE_COMPUTED_EMBEDDINGS).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 166, 2, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(PRE_COMPUTED_EMBEDDINGS).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(CS_ID_to_IDX_MAP) #[\"10-177-V1\"][5][\"graphcodebert_code_embedding\"]\n",
    "with open('code_snippet_id_mapping.json', 'w') as f:\n",
    "    json.dump(CS_ID_to_IDX_MAP, f) #, indent=4)\n",
    "\n",
    "PRE_COMPUTED_EMBEDDINGS = np.array(PRE_COMPUTED_EMBEDDINGS)\n",
    "np.save(\"PRE_COMPUTED_EMBEDDINGS.npy\", PRE_COMPUTED_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10-258-V3': 0,\n",
       " 'A1117-384': 1,\n",
       " '49-510-V1': 2,\n",
       " '142-929-V2': 3,\n",
       " '10-928-V1': 4,\n",
       " '366-143-V1': 5,\n",
       " 'A49-7086': 6,\n",
       " 'A1117-2696': 7,\n",
       " '49-510-V2': 8,\n",
       " '369-1404-V1': 9,\n",
       " 'A34-6': 10,\n",
       " '189-1871-V2': 11,\n",
       " '10-181-V2': 12,\n",
       " '142-929-V1': 13,\n",
       " '49-76-V3': 14,\n",
       " '10-177-V1': 15,\n",
       " '189-1871-V1': 16,\n",
       " '10-258-V2': 17,\n",
       " '10-177-V2': 18,\n",
       " 'A189-895': 19,\n",
       " '10-258-V1': 20,\n",
       " '10-928-V2': 21,\n",
       " 'A84-600': 22}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CS_ID_to_IDX_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 166, 2, 768)\n"
     ]
    }
   ],
   "source": [
    "# Load the array from file\n",
    "loaded_array = np.load(\"PRE_COMPUTED_EMBEDDINGS_5.npy\")\n",
    "print(loaded_array.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
